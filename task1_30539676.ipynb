{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "## Task 1\n",
    "#### Student Name: Anirban Roy Chowdhury\n",
    "#### Student ID: 30539676\n",
    "\n",
    "Date: 13/09/20\n",
    "\n",
    "Version: 3.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* re (for regular expression) \n",
    "* os (for navigating to dir of the file.)\n",
    "* Langid (For language classification)\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "The first part of the assignment, touches upon the first steps of analyzing textual data, i.e extracting data from semi-structured text files.\n",
    "We are given about 2500 text files, containing responses from the twitter API and the data is in JSON format. \n",
    "\n",
    "Our tasks in as follows:\n",
    "* Read & Process all the data.\n",
    "* Crawl through the data to find the required information.\n",
    "* Output the desired infomration in an XML format file, with specific constraints as given in the assignment.\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os,re\n",
    "from langid.langid import LanguageIdentifier, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Examining and Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining a the text file, we can see that it is a JSON file, received from the twitter API. We can confirm this guess by passing the given file through an online JSON viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to read all the data given to us for processing. Using the OS lib, we are able to walk to the required directory. Our files our \"utf-8\" encoded, each entry within the \"new_list\" variable contains all the raw text within the one of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_list = []\n",
    "#OS walk to the required directory, we read the text from each individual files and append the entire tezt into the list\n",
    "for root, dirs, files in os.walk(\"D:\\\\2.Uni Work\\\\FIT5196\\\\Assignment 1\\\\30539676\\\\\"):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            #new_list[re.search(r'\\d{4}-\\d{2}-\\d{2}',str(file)).group()] = text\n",
    "            new_list.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Just by looking at one of the enteries within \"new_list\" we can see that the data within the text files are JSON responses.</p>\n",
    "<p>\n",
    "Since a JSON is nothing but a complicated python dictionary, we can use the in-built \"eval()\" function to return a python dict version of the JSON data stored within the text files.</p>\n",
    "<i>from python doc</i>\n",
    "<p>eval() - The expression argument is parsed and evaluated as a Python expression</p>\n",
    "<p>\n",
    "However here we face a problem, our data has lots of keys, whose value are \"true\" and \"false\", since in python these boolean values are special words and are defined as \"True\" & \"False\". if we directly use eval() to get our python dictionary, it will fail.\n",
    "    \n",
    "One thing to note is that within the twitter data, we have a \"data\" and \"error\" list. The data list contains all tweets, while the error list contains appropriate error logs from the reponses of the twitter API. our assignmentment only focuses on the data part of the responses, as such the error details are irrelevant to us in terms of this assignment.\n",
    "<p>\n",
    "We will be using regex to solve this issue, by replacing all true and false with their capitalized versions, we will be able to use eval() to get our desired output,saving us time by not using regex to manually crawl through the data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Parsing & Extracting the Required Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will intialize all the required classifiers and dictionaries for further computation here.\n",
    "\n",
    "* <b>identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)</b> - This is a Langid object which classifies the language of the tweet, we have normalized the probability so that it comes within the range of 0 to 1.\n",
    "* <b>d = {'true': 'True', 'false': 'False'} </b> - This is a dictionary containing all the words that are to be replaced as keys, with the value being teh words they are to be replaced with.\n",
    "* <b>pattern = re.compile(r\"(true|false)\")</b> - Finally we compile a regex object containing the pattern that we want to find. In our case its <u>\"true|false\"</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initializing the required empty dictionaries and lists\n",
    "wrangled_data = {}\n",
    "dict_sorted_acc_date = {}\n",
    "#Initializing our classified\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "#initializing dicts, containing words to be changed for manipulation. The key is the word to search for and the value is the word to replace it with\n",
    "d = {'true': 'True', 'false': 'False'}\n",
    "#regex to replace the above defined dict\n",
    "pattern = re.compile(r\"(true|false)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Following are the steps taken in the bellow cell to extract the required data as per the assignment.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Iterate over each entry within \"new_list\", which is the raw text data within one file.\n",
    "2. Replace \"true\" and \"false\" with \"True\" and \"False\".\n",
    "3. Using **eval()** get the corresponsing python dict from the text data.\n",
    "4. Iterate over the returned dict from eval().\n",
    "5. When the key matches \"data\", for every item within the \"data\" list of that file.\n",
    "6. Encode the text to utf-16 and then decode it back to utf-8, since according to the assignment specification emoji's are to be displayed as it.\n",
    "7. Since we have been given **utf-8** files, if we dont encode it into **utf-16** and then back, the emoji will be represented as the corresponsing combination of utf-8 charecters to depict the emoji. According to our assignment specification we want the emoji to be represented by the associated image.\n",
    "8. Since **Langid** cannot work in parallel, each text has to be individually passed through our function to get which language it belongs to. Filter out any non-english language.\n",
    "8. All english language tweets are stored in a dictionary where the \"tweet id\" is the key. If there are multiple tweets with same id, the most recent one is kept within the dict.\n",
    "9. We also use regex to get the date from the tweet, while storing it in \"wrangled_data\" dict.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Iterate through each item in the created list\n",
    "for val in new_list:\n",
    "    #Replace our defined words for the raw_text\n",
    "    raw_text = pattern.sub(lambda m: d.get(m.group(1), m.group(1)), val)\n",
    "    #Since ouur input is just a JSON file. We can use eval to get our data into dict format.\n",
    "    evaluated_dict = eval(raw_text)\n",
    "    #Iterate over the dict \n",
    "    for key, value in evaluated_dict .items():\n",
    "        #On condition match\n",
    "        if key == 'data':\n",
    "            #Iterate over the list of values containing all the tweets\n",
    "            for item in value:\n",
    "                #each tweet text id encoded into utf16 surrogate to handle twitter emojis, since in utf16 2 utf8 charecter combine to create a emoji\n",
    "                utf8_utf16_encoded_string = item['text'].encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "                #If the tweet is classified as an english tweet\n",
    "                if identifier.classify(utf8_utf16_encoded_string)[0] == 'en':\n",
    "                    #Add the tweet into a dict along with its created_at date in its required format using regex\n",
    "                    wrangled_data[item['id']] = [utf8_utf16_encoded_string, re.search(r'\\d{4}-\\d{2}-\\d{2}',item['created_at']).group()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The wrangled_data dict contains a list of value, where the value on first index is the twitter text and the value on second index is the date.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Since we have to write our data to an xml file, we will reformat our dict \"wrangled_dict\" into another dict, which is has its <u>key</u> as the Date and is sorted ascendingly accodring to it. </p>\n",
    "<p>Now in this new dict each Key i.e date has a list of value, within this list there are multiple lists each containing the ID & text of a single tweet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We reformat our wrangled_data dictionary into a more manageble format for printing to an .xml file, by grouping all the tweets accoriding to their created_at date\n",
    "for key, value in sorted(wrangled_data.items()):\n",
    "    dict_sorted_acc_date.setdefault(value[1], []).append([key,value[0]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Writing XML files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating over the \"dict_sorted_acc\" we can output our desired xml files, encoded in utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Write to the desired outputfile according to the given schema\n",
    "with open('30539676.xml','w+',encoding='utf-8') as f:\n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    f.write('<data>\\n')\n",
    "    for key, val in dict_sorted_acc_date.items():\n",
    "        f.write('<tweets date=\"%s\">\\n' % (key))\n",
    "        for item in val:\n",
    "            f.write('<tweet id=\"%s\">%s</tweet>\\n' % (item[0],item[1]))\n",
    "        f.write('</tweets>\\n')\n",
    "    f.write('</data>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "<p>\n",
    "Following tasks have been completed:\n",
    "    \n",
    "    1. Reading all .txt files given.\n",
    "    2. Replacing \"true\" & \"false\" with True and False.\n",
    "    3. Using eval() to get a python dict to work upon.\n",
    "    4. Filtering the required non-english tweets using Langid.\n",
    "    5. Converting utf-8 encoded charecter to utf-16 then back, to preserve emojis.\n",
    "    6. Creating a dict containing the relevant information\n",
    "    7. Generating the required XML file.\n",
    "    \n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "## Task 2\n",
    "#### Student Name: Anirban Roy Chowdhury\n",
    "#### Student ID: 30539676\n",
    "\n",
    "Date: 13/09/20\n",
    "\n",
    "Version: 3.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* re (for regular expression, included in Anaconda Python 3.6) \n",
    "* os (for )\n",
    "* nltk (Natural language toolkit)\n",
    "* pandas (for data frame, included in Anaconda Python 3.6)\n",
    "* langid (for language classification)\n",
    "* nltk.tokenize (for tokenization, both MWE and single word)\n",
    "* nltk.stem (for porter stemmer)\n",
    "* itertools (for iteration methods)\n",
    "* nltk.collocations (for finding bigrams, and unigrams)\n",
    "* nltk.probability ()\n",
    "* sklearn.feature_extraction.text (for CounterVectorizatoin, to generate sparse matrix representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "For this task we have multiple steps that need to be executed. \n",
    "\n",
    "<p>Our goal is to generate:\n",
    "\n",
    "1. 100uni.txt - A file containing top 100 unigrams according to each day along with its count.\n",
    "2. 100bi.txt - A file containing top 100 bigrams according to each day along with its count, set according to certain measures.\n",
    "3. vocab.txt - A text file containing all the word within our corpus. Each word is assigned a unique index to e referred with.\n",
    "4. countVec.txt - A text file containing the sparse doc matrix representation, where each sheet is displayed as a string of numbers, where the key is the unique index referred to the word in vocab.txt and the value is the frequency of the word.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "A general run down of the steps of how we will be achieving this is:\n",
    "\n",
    "1. Read and clean all our excel data sheets.\n",
    "2. Identify and remove all non english tweets.\n",
    "3. tokenize the text.\n",
    "4. Remove context dependent and context independent stop words.\n",
    "5. Porter Stemming of each token.\n",
    "6. Generation of top 100 unigrams and bigrams per day(sheet).\n",
    "7. Create Sparse matrix representation of the document\n",
    "8. Generation of vocab and counVec.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os,re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import itertools\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Methodology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.Data Exploration & Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Initialize variables\n",
    "\n",
    "<p>\n",
    "We initialize out language identifier, the given regex and our porter stemmer. These will be used later on in the assignment, but are initialized here. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing our classifier\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "#Intialize tokenizer\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "#Porter Stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Read Data & Create context independent stop word list \n",
    "\n",
    "<p>\n",
    "We can read the entire excel workbook along with all the sheets by passing \"sheet_name=None\" .\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read all the sheets in the excel file without using the excel headers\n",
    "#excel_data_set = pd.read_excel('D:\\\\2.Uni Work\\\\FIT5196\\\\Assignment 1\\\\30539676.xlsx', sheet_name = None, header= None)\n",
    "excel_data_set = pd.read_excel('D:\\\\2.Uni Work\\\\FIT5196\\\\Assignment 1\\\\part2\\\\sample.xlsx', sheet_name = None, header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating a list of alll stop words\n",
    "stop_words = []\n",
    "#open the file\n",
    "with open('stopwords_en.txt', 'r', encoding='utf8') as f:\n",
    "    #Iterate over each line while stripping '\\n' from the text and adding it into the List\n",
    "    stop_words = [line.lower().rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Initial Manipulation (Cleaning and removing useless data)\n",
    "<p>\n",
    "Since our data has multiple null rows and column, duplicate tweets and non-enlgish tweets, we will need to do a thorough cleaning of the data before we can start with out text analysis.\n",
    "</p> \n",
    "\n",
    "<p>\n",
    "There are multiple steps in our wrangling steps:\n",
    "\n",
    "1. Since each sheet within the excel, does not have the table of data properly placed according to the starting indexes, we will need to drop all useless null values both on the column and row axis.\n",
    "2. We create our inital 3 column and remove the header.\n",
    "3. Drop any duplicate tweets by 'id' column and keep only the last occurance.\n",
    "4. Lower case the text column.\n",
    "5. Since Langid cannot be parallelized thorugh apply function, iterate over each row and classify each text according to their langugage.\n",
    "6. Drop all rows where the \"lang\" column does not have english as its classified language.\n",
    "7. Tokenize the remaining tweets and add the tokens to the \"Token\" column.\n",
    "8. Remove all context-independent stop words from the \"token\" column.\n",
    "9. remove all tokens with length less than 3.\n",
    "10. Reset Index.\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unigrams_dict = {}\n",
    "bigrams_dict = {}\n",
    "#iterate over the dict containing the name of the sheet as key and dataframe containing the sheet data as value\n",
    "for key, df in excel_data_set.items():\n",
    "    #Dropping columns with null values\n",
    "    df.dropna(how=\"all\",axis=1,inplace= True)    \n",
    "    #Dropping rows with null values\n",
    "    df.dropna(subset=df.columns,inplace= True)\n",
    "    #Changing columns names for easier manipulation and readability \n",
    "    df.columns = ['text','id','created_at']\n",
    "    #Removing header from our dataframes - i.e text, id, created_at tags present in the sheet\n",
    "    df.drop(df.head(1).index, inplace=True)\n",
    "    #Remove duplicates\n",
    "    df.drop_duplicates('id',keep='first',inplace=True)\n",
    "    # Adding intermediary columns\n",
    "    df['lang'] = \"\"\n",
    "    df['token'] = \"\"\n",
    "    df['tokens_minus_context_independent_stop_words'] = \"\"\n",
    "    df['porter_stemmed_token'] = \"\"\n",
    "    df['bigram_token'] = \"\"\n",
    "    df['concatenated_tokens'] = \"\"\n",
    "    df['tokens_minus_all_stop_words'] = \"\"\n",
    "    #Lower case and cast to string\n",
    "    df['text'] = df['text'].apply(lambda x:str(x).lower())\n",
    "    #Iterrate over each tweet and add the classified language to the 'lang' column\n",
    "    for index, row in df.iterrows():\n",
    "        row['lang'] = identifier.classify(row['text'])[0]\n",
    "    #Drop all rows without english as langugage in the created column\n",
    "    df.drop(df.loc[df['lang']!='en'].index, inplace=True)\n",
    "    #Add a column with tthe tokenized result of text row.\n",
    "    df['token'] = df['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    #Only keep those words not in stop_word list\n",
    "    df['tokens_minus_context_independent_stop_words'] = df['token'].apply(lambda x: [token for token in x if token not in stop_words])\n",
    "    df['tokens_minus_context_independent_stop_words'] = df['tokens_minus_context_independent_stop_words'].apply(lambda x: [token for token in x if len(token)>=3])\n",
    "    #df.drop('lang', axis=1, inplace=True)\n",
    "    #Reset index for easier manipulation\n",
    "    df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Creating Context-dependent Stopword list\n",
    "\n",
    "<p>\n",
    "We have now cleaned all the useless information, previously we had created a list of context-Independent stop words, now we will be creating the context-dependent stopword list, using the rules stated in the specification:\n",
    "\n",
    "* Words appearing in more than 60days(sheet)\n",
    "* Rare tokens which are appearing in fewer than 5days(sheet)\n",
    "\n",
    "Since we have already tokenized our text and removed context-independent stop words, we can use the below code to get the count of days(sheet) a word appears in.\n",
    "\n",
    "We create a list of all words within each day(sheet) per sheet, then create a set of each list, now eah list has only the unique words within that day(sheet). If we get the freqdist of this list, it will give us the count of days(sheet) each unique word has appeared in.\n",
    "\n",
    "By filtering this list according to our condition we get the lists of context-dependent stopwords.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Iterate over excel_sheet_data\n",
    "#Each df column is converted to a list\n",
    "#Then only the unique values are kept\n",
    "#Chain it into a list, Chain.from_iteratbles creates a list of list containing each list of tokens.\n",
    "#Now on getting the freqDist, since each list is representing the unique words within the file i.e, date\n",
    "#The freq will give us the number of documents the unique word has appeared in.\n",
    "words_2 = list(chain.from_iterable([set(list(chain(*df['tokens_minus_context_independent_stop_words']))) for key, df in excel_data_set.items()]))\n",
    "fd_2 = FreqDist(words_2)\n",
    "words_greater_60 = list(filter(lambda x:fd_2[x]>60,fd_2))\n",
    "words_lesser_5 = list(filter(lambda x:fd_2[x]<5,fd_2))\n",
    "context_dependent_stopwords = words_greater_60 + words_lesser_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lesser_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the frequencies of few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_2.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Final Manipulation (Remove Context dependent words, porter steeming & concatination.\n",
    "\n",
    "<p>\n",
    "Following are the steps:\n",
    "\n",
    "1. remove any words that are appearing in our context-dependent stop words lists (i.e words occuring in greater than 60days & less than 5 days)\n",
    "2. Porter stem all words.\n",
    "3. We have also concatinated all the tokens for each tweet back into a single string, this is mostly for countervectorization that we will be doing further below, this rows also gives us a basic gist of each tweet devoid of any stopwords and only showing those words, which are relevant to textual analysis.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, df in excel_data_set.items():\n",
    "#     #Porter stem\n",
    "    print(key)\n",
    "    df['porter_stemmed_token'] = df['tokens_minus_context_independent_stop_words'].apply(lambda x:[stemmer.stem(token) for token in x if token not in context_dependent_stopwords])\n",
    "    #Concatinating all the porter stemmed token together per row.\n",
    "    df['concatenated_tokens'] = df['porter_stemmed_token'].apply(lambda x: \" \".join(x))\n",
    "    print(df['concatenated_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.Bigram & Unigram Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function taken from tutorial to return a list of top 100 bigrams.\n",
    "def get_bigrams(input_list,pmi_measure=100):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(input_list)\n",
    "    bigram_finder.apply_freq_filter(20)\n",
    "    #bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "    top_n_bigrams = bigram_finder.nbest(bigram_measures.pmi, pmi_measure) # Top-100 bigrams\n",
    "    return top_n_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Following are the steps executed to get our unigram & bigram dict:\n",
    "\n",
    "1. Each entry into the unigram_dict has its key as the datasheet name (i.e, date) and value as the sorted list of Frequency distribution of the porter stemmed token on that sheet.\n",
    "2. Per sheet we generate a list of top 100 bigram tokens using our get_bigram() function, which return these values based on the parameters mentioned in the specification.\n",
    "3. We initialize a MWE tokenizer per sheet.\n",
    "4. The MWE tokenizer is then used to generate bigrams by applying it on the 'token' column, note this column only has the token - no pre processing is done on this column. The result are then stored in 'bigram_token' column.\n",
    "5. Since MWE tokenizer joins 2 unigram into a bigram using the default seperator '_', we will filter the 'bigram_token' column, only leaving in those column with '_'.\n",
    "6. Finally similar to the unigram_dict, make an entry into the bigram_dict containing the frequency distribution of the bigrams generated per sheet.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, df in excel_data_set.items():\n",
    "    #add entry into unigram dict from porter stemmed tokens for a single sheet.\n",
    "    unigrams_dict[key] = sorted(list(dict(FreqDist(list(chain(*df['porter_stemmed_token'].tolist())))).items()),key=lambda x:x[1],reverse=True)[:100]\n",
    "    #####################BIGRAMS#####################\n",
    "    #Get 100 bigrams per sheet according to paramets in function.\n",
    "    top_100_bigrams = get_bigrams(list(chain(*df['token'].tolist())),100)\n",
    "    #Intialize the MWE tokenizer for the current shet by passing the top 100 bigrams\n",
    "    mwetokenizer = MWETokenizer(top_100_bigrams)\n",
    "    #Apply the trained bigrams on the token column to generate bigrams\n",
    "    df['bigram_token'] = df['token'].apply(lambda x: mwetokenizer.tokenize(x))\n",
    "    #only keep the words with \"_\" in them, since MWE join bigrams using \"_\"\n",
    "    df['bigram_token'] =  df['bigram_token'].apply(lambda x: [token for token in x if '_' in token])\n",
    "    #add entry into the dictionary of bigrams containing the top 100 bigram of the sheet.\n",
    "    bigrams_dict[key] = sorted(list(dict(FreqDist(list(chain(*df['bigram_token'].tolist())))).items()),key=lambda x:x[1],reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_key = list(excel_data_set.keys()).pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data_set[test_key]['text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data_set['2020-03-22']['concatenated_tokens'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is just to reformat our dictionary into the format of the assignment specification, it just splits the joined tuple, into the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reformat our dictionary into our desired output format\n",
    "new_bigram_dict = {}\n",
    "for key, value in bigrams_dict.items():\n",
    "    temp = []\n",
    "    for x in value:\n",
    "        word_split = x[0].split(\"_\")\n",
    "        a = ((word_split[0],word_split[1]),x[1])\n",
    "        temp.append(a)\n",
    "    new_bigram_dict[key] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output code for creating the required _100uni.txt and _100bi.txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Write to the desired outputfile according to the given schema\n",
    "with open('30539676_100uni.txt','w+',encoding='utf-8') as f:\n",
    "    for key, val in unigrams_dict.items():\n",
    "        f.write('%s:%s\\n' % (key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to the desired outputfile according to the given schema\n",
    "with open('30539676_100bi.txt','w+',encoding='utf-8') as f:\n",
    "    for key, val in new_bigram_dict.items():\n",
    "        f.write('%s:%s\\n' % (key,val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Creating sparse matrix to generate countvec.txt and vocab.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below 2 code cells we are initializing our CountVectorizer and fitting it over a list of list, where each inner list containg the entire vocbulary corpus of a sheet, and the outer list containg lists with each list representating a document (sheet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sample vocab and count vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_features = vectorizer.fit_transform([\" \".join(df['concatenated_tokens'].tolist()) for key,df in excel_data_set.items()])\n",
    "print (data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a counter and then iterativly defined a dictionary with \"key\" as the word and \"value\" as the index counter. The output is our document-by-word matrix dimensions. In our case since our excel sheet has 81 sheets. It will be 81*no_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dict for vocab index, each word is assigned a unique index.\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab_index_dict = {}\n",
    "counter = 0\n",
    "for word in vocab:\n",
    "    vocab_index_dict[word] = counter\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The following code cell creates the doc-word matrix where the key is the name of the sheet(in our case the date), while the value is a dictionary which contains the words that appear in that sheet along with their counts. \n",
    "\n",
    "Following are the steps taken:\n",
    "\n",
    "1. Creat a list of keys from excel_data_set\n",
    "2. Get the vocab list from \"vectorizer.get_feature_names()\"\n",
    "3. iterate over each row of the 2D array returned by \"data_features.toarray()\".\n",
    "    1. Here each row is the doc-word representation of a sheet.\n",
    "4. Now by using zip() function we can zip together the vocab and and the doc-word representation of that sheet.\n",
    "    1. Since the vocab is sorted in the same order as the array representation returned by \"data_features.toarray()\", zip will correctly combine the word and the letter.\n",
    "5. Add the dict word_count_dict created as a value to doc_matrix_dict with key being the date. \n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "date_keys = list(excel_data_set.keys())\n",
    "word_count_dict ={}\n",
    "doc_matrix_dict = {}\n",
    "for doc_matrix_rep in data_features.toarray():\n",
    "    word_count_dict = {}\n",
    "    date_key = date_keys.pop(0)\n",
    "    for word, count in zip(vocab,doc_matrix_rep):\n",
    "        if count>0:\n",
    "            word_count_dict[word] = count\n",
    "    doc_matrix_dict[date_key] = word_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The folllowing code is used to create the countvec representation that is required, The below code cell replaces each occurance of word with its corresponsing unique id from _vocab.txt. This way were are able to create a countVec representation of each document(sheet).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating output for countvec.txt\n",
    "reformated_doc_matrix_date = {}\n",
    "for key,val in doc_matrix_dict.items():\n",
    "    word_keys = val.keys()\n",
    "    temp_dict = dict()\n",
    "    for word in word_keys:\n",
    "        if word in vocab_index_dict:\n",
    "            temp_dict[vocab_index_dict[word]] = val[word]\n",
    "    reformated_doc_matrix_date[key] = temp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for creating the required files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to write vocab.txt\n",
    "with open('30539676_vocab.txt','w+',encoding='utf-8') as f:\n",
    "    for key,val in vocab_index_dict.items():\n",
    "        f.write('%s:%s\\n'%(key,vocab_index_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('30539676_countvec.txt','w+') as f:\n",
    "    for key, val in reformated_doc_matrix_date.items():\n",
    "        f.write('%s,%s\\n' % (key,str(val).strip('{}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "<p>\n",
    "Following tasks have been done:\n",
    "    \n",
    "    1. Parsed the data, dropped null and duplicate values.\n",
    "    2. Keep only english text\n",
    "    3. Tokenize\n",
    "    4. Remove context independent and dependent words\n",
    "    5. Porter stemmed eah token\n",
    "    6. Generated top 100 unigrams and bi grams per sheet\n",
    "    7. Generated vocab of the corpus\n",
    "    8. Generated doc-word matrix representation through CounterVectorizer\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}